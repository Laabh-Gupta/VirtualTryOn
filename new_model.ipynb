{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Imports and Device Setup\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Section 2: Model Definitions\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(9, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/kaggle/input/high-resolution-viton-zalando-dataset/train\\\\image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Define transformations\u001b[39;00m\n\u001b[0;32m     34\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     35\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m192\u001b[39m)),\n\u001b[0;32m     36\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m     37\u001b[0m ])\n\u001b[1;32m---> 39\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mVirtualTryOnDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/high-resolution-viton-zalando-dataset/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mVirtualTryOnDataset.__init__\u001b[1;34m(self, root_dir, transform)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir \u001b[38;5;241m=\u001b[39m root_dir\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/kaggle/input/high-resolution-viton-zalando-dataset/train\\\\image'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Section 3: Dataset Preparation\n",
    "class VirtualTryOnDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(os.path.join(root_dir, 'image'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load images from the respective directories\n",
    "            image = Image.open(os.path.join(self.root_dir, 'image', img_name))\n",
    "            cloth = Image.open(os.path.join(self.root_dir, 'cloth', img_name))\n",
    "            cloth_mask = Image.open(os.path.join(self.root_dir, 'cloth-mask', img_name))\n",
    "            agnostic = Image.open(os.path.join(self.root_dir, 'agnostic-v3.2', img_name))\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                cloth = self.transform(cloth)\n",
    "                cloth_mask = self.transform(cloth_mask)\n",
    "                agnostic = self.transform(agnostic)\n",
    "\n",
    "            return image, cloth, cloth_mask, agnostic\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {img_name} not found. Skipping.\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # Skip to the next item if file is missing\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = VirtualTryOnDataset(root_dir='/kaggle/input/high-resolution-viton-zalando-dataset/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Model Initialization\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "criterion_gan = nn.BCELoss()\n",
    "criterion_l1 = nn.L1Loss()\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Section 5: Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (image, cloth, cloth_mask, agnostic) in enumerate(train_loader):\n",
    "        \n",
    "        # Move data to GPU if available\n",
    "        image = image.to(device)\n",
    "        cloth = cloth.to(device)\n",
    "        cloth_mask = cloth_mask.to(device)\n",
    "        agnostic = agnostic.to(device)\n",
    "        \n",
    "        # Generate fake images using the generator\n",
    "        warped_cloth = cloth  # Simplified step (youâ€™d warp it in practice)\n",
    "        input_to_generator = torch.cat((agnostic, warped_cloth), dim=1)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        # Real images\n",
    "        real_input = torch.cat((image, agnostic, warped_cloth), dim=1)\n",
    "        real_output = discriminator(real_input)\n",
    "        real_label = torch.ones_like(real_output).to(device)\n",
    "        loss_d_real = criterion_gan(real_output, real_label)\n",
    "        \n",
    "        # Fake images\n",
    "        fake_image = generator(input_to_generator)\n",
    "        fake_input = torch.cat((fake_image, agnostic, warped_cloth), dim=1)\n",
    "        fake_output = discriminator(fake_input.detach())\n",
    "        fake_label = torch.zeros_like(fake_output).to(device)\n",
    "        loss_d_fake = criterion_gan(fake_output, fake_label)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        loss_d = (loss_d_real + loss_d_fake) / 2\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        fake_output = discriminator(fake_input)\n",
    "        loss_g_gan = criterion_gan(fake_output, real_label)\n",
    "        loss_g_l1 = criterion_l1(fake_image, image) * 10.0  # L1 loss for pixel-level alignment\n",
    "        \n",
    "        # Total generator loss\n",
    "        loss_g = loss_g_gan + loss_g_l1\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # Print loss every few iterations\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], \"\n",
    "                  f\"D Loss: {loss_d.item():.4f}, G Loss: {loss_g.item():.4f}\")\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(generator.state_dict(), f\"/kaggle/working/generator_epoch_{epoch+1}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"/kaggle/working/discriminator_epoch_{epoch+1}.pth\")\n",
    "    print(f\"Model saved for epoch {epoch+1}\")\n",
    "\n",
    "    # Display a sample output image\n",
    "    with torch.no_grad():\n",
    "        sample_fake_image = fake_image[0].cpu().permute(1, 2, 0) * 0.5 + 0.5  # Re-normalize for display\n",
    "        sample_fake_image = np.clip(sample_fake_image.numpy(), 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(sample_fake_image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Generated Image after Epoch {epoch+1}\")\n",
    "        plt.show()\n",
    "\n",
    "# Section 6: Training Complete\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3855472,
     "sourceId": 6683799,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
