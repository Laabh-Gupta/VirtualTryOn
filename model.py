# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14EkfLMrTvN-R1GYbVEIkDwGuXWTtKWb9
"""

import os
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
import matplotlib.pyplot as plt

# Define transforms
transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize images to 128x128
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# Custom dataset class
class FashionDataset(Dataset):
    def __init__(self, img_paths, transform=None):
        self.img_paths = img_paths
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        image = Image.open(img_path).convert("RGB")  # Ensure image is RGB
        if self.transform:
            image = self.transform(image)
        return image

# Path to the dataset folder
dataset_folder = "test/image"

# Get a list of all image paths
img_paths = [os.path.join(dataset_folder, img) for img in os.listdir(dataset_folder) if img.endswith(".jpg")]

# Limit to a smaller sample (e.g., the first 500 images)
sample_size = 10
img_paths = img_paths[:sample_size]

# Create dataset and dataloader
dataset = FashionDataset(img_paths, transform=transform)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # Reduced batch size for faster training

# Attention Module
class AttentionModule(nn.Module):
    def __init__(self, in_channels, reduction=8):
        super(AttentionModule, self).__init__()
        self.query = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1)
        self.key = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1)
        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        batch_size, C, H, W = x.size()
        query = self.query(x).view(batch_size, -1, H * W)  # Bx(reduced_C)xHW
        key = self.key(x).view(batch_size, -1, H * W)  # Bx(reduced_C)xHW
        value = self.value(x).view(batch_size, -1, H * W)  # BxCxHW

        attention = torch.bmm(query.permute(0, 2, 1), key)  # BxHWxHW (reduced)
        attention = self.softmax(attention)
        out = torch.bmm(value, attention.permute(0, 2, 1))  # BxCxHW
        out = out.view(batch_size, C, H, W)
        return out

class VirtualTryOnModel(nn.Module):
    def __init__(self):
        super(VirtualTryOnModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.attention = AttentionModule(128, reduction=8)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)

        # Calculate output size after convolutions and pooling
        sample_input = torch.zeros(1, 3, 128, 128)  # Batch size of 1, 3 channels, 128x128 size
        self.flattened_size = self._get_flattened_size(sample_input)

        self.fc = nn.Linear(self.flattened_size, 10)  # Adjust based on the output size

    def _get_flattened_size(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = self.attention(x)
        x = torch.relu(self.conv3(x))
        return x.numel()  # Get total number of elements in the tensor

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = self.attention(x)
        x = torch.relu(self.conv3(x))
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc(x)
        return x

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VirtualTryOnModel().to(device)

# Adjust the class counts based on your dataset
class_counts = [100, 200, 150, 120, 130, 110, 140, 160, 180, 150]  # Example counts for 10 classes
class_weights = 1. / torch.tensor(class_counts, dtype=torch.float32)

# Modify this line to ensure class_weights has the correct shape
criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))  # Apply class weights

optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adjust learning rate

# Mixed precision training setup
scaler = GradScaler()  # Updated initialization

# Training Loop
num_epochs = 10
accumulation_steps = 4  # Accumulate gradients over 4 mini-batches

# To store loss and accuracy for visualization
losses = []
accuracies = []

for epoch in range(num_epochs):
    running_loss = 0.0
    correct_predictions = 0
    total_samples = 0
    optimizer.zero_grad()  # Reset gradients at the start of each epoch
    print(f'Starting Epoch {epoch+1}/{num_epochs}')

    for i, images in enumerate(dataloader):
        images = images.to(device)  # Move images to GPU

        # Generate random labels for demonstration (replace with actual labels in practice)
        labels = torch.randint(0, 10, (images.size(0),)).to(device)  # Move labels to GPU

        with autocast():  # Enable mixed precision
            outputs = model(images)
            loss = criterion(outputs, labels)

        # Scale loss and perform backward pass
        scaler.scale(loss).backward()

        # Perform optimization step every 'accumulation_steps' mini-batches
        if (i + 1) % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()  # Reset gradients after the step

        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        correct_predictions += (predicted == labels).sum().item()
        total_samples += labels.size(0)

        # Print statistics
        running_loss += loss.item()
        if i % 10 == 9:  # Print every 10 batches
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {running_loss/10:.4f}')
            running_loss = 0.0

    # Calculate epoch loss and accuracy
    epoch_loss = running_loss / len(dataloader) if len(dataloader) > 0 else 0
    epoch_accuracy = correct_predictions / total_samples * 100 if total_samples > 0 else 0
    losses.append(epoch_loss)
    accuracies.append(epoch_accuracy)

    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')

print('Finished Training')

# Visualization of Loss and Accuracy
plt.figure(figsize=(12, 5))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(losses, label='Training Loss', color='blue')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(accuracies, label='Training Accuracy', color='orange')
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()

plt.tight_layout()
plt.show()

