# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_SndoayNOopahXVybcgEDpB-ytV_uDyb
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# Define augmentations and normalization
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]
])

import os
from PIL import Image
from torch.utils.data import Dataset

class FashionDataset(Dataset):
    def __init__(self, person_img_paths, cloth_img_paths, transform=None):
        # Filter out invalid paths
        valid_indices = []
        for i in range(len(person_img_paths)):
            if os.path.exists(person_img_paths[i]) and os.path.exists(cloth_img_paths[i]):
                valid_indices.append(i)

        self.person_img_paths = [person_img_paths[i] for i in valid_indices]
        self.cloth_img_paths = [cloth_img_paths[i] for i in valid_indices]
        self.transform = transform

    def __len__(self):
        return len(self.person_img_paths)

    def __getitem__(self, idx):
        person_path = self.person_img_paths[idx]
        cloth_path = self.cloth_img_paths[idx]

        try:
            person_img = Image.open(person_path).convert("RGB")
            cloth_img = Image.open(cloth_path).convert("RGB")
        except Exception as e:
            print(f"Error loading images: {e}")
            return None, None  # This won't happen due to filtering

        if self.transform:
            person_img = self.transform(person_img)
            cloth_img = self.transform(cloth_img)

        return person_img, cloth_img

# Define Warping Network (simplified for demonstration)
class WarpingNetwork(nn.Module):
    def __init__(self):
        super(WarpingNetwork, self).__init__()
        self.warp = nn.Sequential(
            nn.Conv2d(6, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 3, kernel_size=3, padding=1)
        )

    def forward(self, person_img, cloth_img):
        x = torch.cat([person_img, cloth_img], dim=1)
        warped_cloth = self.warp(x)
        return warped_cloth

# Define Warping Network (simplified for demonstration)
class WarpingNetwork(nn.Module):
    def __init__(self):
        super(WarpingNetwork, self).__init__()
        self.warp = nn.Sequential(
            nn.Conv2d(6, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 3, kernel_size=3, padding=1)
        )

    def forward(self, person_img, cloth_img):
        x = torch.cat([person_img, cloth_img], dim=1)
        warped_cloth = self.warp(x)
        return warped_cloth

# Define the GAN for blending the warped cloth onto the person
class TryOnGAN(nn.Module):
    def __init__(self):
        super(TryOnGAN, self).__init__()
        self.generator = nn.Sequential(
            nn.Conv2d(6, 512, kernel_size=3, padding=1),
            nn.LeakyReLU(0.4),
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.LeakyReLU(0.4),
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.LeakyReLU(0.4),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.LeakyReLU(0.4),
            nn.Conv2d(64, 3, kernel_size=3, padding=1)
        )
        self.discriminator = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(128 * 32 * 32, 1),
            nn.Sigmoid()
        )

    def generate(self, person_img, warped_cloth):
        x = torch.cat((person_img, warped_cloth), dim=1)
        return self.generator(x)

    def discriminate(self, tryon_img):
        return self.discriminator(tryon_img)

# Training Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize the networks
warp_net = WarpingNetwork().to(device)
gan_net = TryOnGAN().to(device)

# Optimizers and Loss functions
warp_optimizer = optim.Adam(warp_net.parameters(), lr=0.0001)
gen_optimizer = optim.Adam(gan_net.generator.parameters(), lr=0.0001)
disc_optimizer = optim.Adam(gan_net.discriminator.parameters(), lr=0.0001)
bce_loss = nn.BCELoss()
import os

# Load Dataset
dataset_folder = "test/classified"  # Use '/' or escape '\' in paths
person_folder = os.path.join(dataset_folder, 'person')
cloth_folder = os.path.join(dataset_folder, 'cloth')

# Get list of image paths for both person and cloth images
person_img_paths = [os.path.join(person_folder, img) for img in os.listdir(person_folder) if img.endswith(('.png', '.jpg', '.jpeg'))]
cloth_img_paths = [os.path.join(cloth_folder, img) for img in os.listdir(cloth_folder) if img.endswith(('.png', '.jpg', '.jpeg'))]

# Print counts and some paths for verification
print(f"Total person images found: {len(person_img_paths)}")
print(f"Total cloth images found: {len(cloth_img_paths)}")
print(f"Example person image path: {person_img_paths[:5]}")
print(f"Example cloth image path: {cloth_img_paths[:5]}")

# Continue with dataset creation
dataset = FashionDataset(person_img_paths, cloth_img_paths, transform=transform)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# Log the number of images used
print(f"Loaded {len(dataset)} images into the dataset.")

# Initialize lists to store losses
# Initialize lists to store losses and collected images for display
disc_losses = []
gen_losses = []
person_images = []
warped_clothes = []
generated_tryons = []

# Set up environment to avoid some common errors
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

num_epochs = 500
for epoch in range(num_epochs):
    epoch_disc_loss = 0.0
    epoch_gen_loss = 0.0

    for batch in tqdm(dataloader):
        person_img, cloth_img = batch

        if person_img is None or cloth_img is None:  # Skip if there was an error loading images
            continue

        person_img, cloth_img = person_img.to(device), cloth_img.to(device)

        # Warp the cloth image
        warped_cloth = warp_net(person_img, cloth_img)

        # Generate the try-on output
        fake_tryon = gan_net.generate(person_img, warped_cloth)

        # Train Discriminator
        disc_optimizer.zero_grad()
        real_labels = torch.ones(person_img.size(0), 1, device=device)
        fake_labels = torch.zeros(person_img.size(0), 1, device=device)

        real_loss = bce_loss(gan_net.discriminate(person_img), real_labels)
        fake_loss = bce_loss(gan_net.discriminate(fake_tryon.detach()), fake_labels)
        disc_loss = real_loss + fake_loss
        disc_loss.backward()
        disc_optimizer.step()

        # Train Generator
        gen_optimizer.zero_grad()
        gen_loss = bce_loss(gan_net.discriminate(fake_tryon), real_labels)
        gen_loss.backward()
        gen_optimizer.step()

        # Accumulate losses for tracking
        epoch_disc_loss += disc_loss.item()
        epoch_gen_loss += gen_loss.item()

    # Average the losses over the number of batches
    avg_disc_loss = epoch_disc_loss / len(dataloader)
    avg_gen_loss = epoch_gen_loss / len(dataloader)

    # Append to the lists for visualization
    disc_losses.append(avg_disc_loss)
    gen_losses.append(avg_gen_loss)

    print(f"Epoch [{epoch + 1}/{num_epochs}] - Discriminator Loss: {avg_disc_loss:.4f}, Generator Loss: {avg_gen_loss:.4f}")

    # Collect images periodically for end-of-training display
    if epoch % 50 == 0 or epoch == num_epochs - 1:
        person_images.append(person_img[0].detach().cpu().permute(1, 2, 0).numpy())
        warped_clothes.append(warped_cloth[0].detach().cpu().permute(1, 2, 0).numpy())
        generated_tryons.append(fake_tryon[0].detach().cpu().permute(1, 2, 0).numpy())

print("Training Complete")

# Plot the losses on the same graph
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
plt.figure(figsize=(10, 5))
plt.plot(disc_losses, label='Discriminator Loss', color='red')
plt.plot(gen_losses, label='Generator Loss', color='blue')
plt.title('Generator and Discriminator Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Display a 4x4 grid of collected input and output images
def plot_image_grid(images, title):
    plt.figure(figsize=(10, 10))
    for i, img in enumerate(images[:16]):
        plt.subplot(4, 4, i + 1)
        plt.imshow(img)
        plt.axis('off')
    plt.suptitle(title)
    plt.show()

# Show collected person images, warped clothes, and generated try-on results
plot_image_grid(person_images, "Person Images (Input)")
plot_image_grid(warped_clothes, "Warped Clothes (Input)")
plot_image_grid(generated_tryons, "Generated Try-On (Output)")

# Visualize results after training
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

def visualize_results(person_img, cloth_img, generated_img):
    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    axes[0].imshow(person_img.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)
    axes[0].set_title("Person Image")
    axes[1].imshow(cloth_img.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)
    axes[1].set_title("Cloth Image")
    axes[2].imshow(generated_img.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)
    axes[2].set_title("Generated Try-On Image")
    for ax in axes:
        ax.axis("off")
    plt.show()
count = 0
# Display a few examples
with torch.no_grad():
    for person_img, cloth_img in dataloader:
        if person_img is None or cloth_img is None:
            continue
        person_img, cloth_img = person_img.to(device), cloth_img.to(device)
        warped_cloth = warp_net(person_img, cloth_img)
        fake_tryon = gan_net.generate(person_img, warped_cloth)
        visualize_results(person_img[0], cloth_img[0], fake_tryon[0])
        break# Show only one example

